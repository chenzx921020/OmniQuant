import torch,os,sys,torch.nn as nn,torch.nn.functional as F,easydict
sys.path.append("../")
from transformers import AutoTokenizer,AutoModelForCausalLM
from quantize.quantizer import UniformAffineQuantizer
from quantize.utils import smooth_ln_fcs_inplace,smooth_fc_fc_inplace,smooth_q_k_inplace,\
                            set_quant_state,register_scales_and_zeros,smooth_and_quant_inplace_quantrainer
from easydict import EasyDict
from quantize.int_linear import QuantLinear
from quantize.int_matmul import QuantMatMul
import datasets,torch,torch.nn as nn,tqdm,random,transformers
from datasets import load_dataset
from transformers import AutoTokenizer
from hx.eval_ppl import eval_ppl
from lm_eval import evaluator
from tqdm import tqdm
from models.int_llama_layer import QuantLlamaDecoderLayer
import utils
from models.LMClass import LMClass
import argparse
import gc
import numpy as np
from datautils import get_loaders

def chat(model,tokenizer,question="who are u?",max_length=128):
    instruction = """[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible,.
            \n<</SYS>>\n\n{} [/INST]"""
    format_input = instruction.format(question)
    token = tokenizer(format_input, return_tensors="pt")
    output = model.generate(token['input_ids'].to(model.device), do_sample=True, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print (response)
    return response

@torch.no_grad()
def evaluate(lm, args,dev):
    results = {}
    if args.multigpu:
        if "opt" in args.net.lower():
            map_layers_to_multi_gpus(lm.model.model.decoder.layers)
            input_device = lm.model.model.decoder.layers[0].device
            output_device = lm.model.model.decoder.layers[-1].device
            lm._device = input_device
            assert input_device == output_device
            lm.model.model.decoder.embed_positions.to(input_device)
            lm.model.model.decoder.embed_tokens.to(input_device)
            lm.model.model.decoder.final_layer_norm.to(output_device)
            lm.model.lm_head.to(output_device)

        elif "llama" in args.net.lower() or "mixtral" in args.net.lower():
            map_layers_to_multi_gpus(lm.model.model.layers)
            input_device = lm.model.model.layers[0].device
            output_device = lm.model.model.layers[-1].device
            assert input_device == output_device
            lm._device = input_device
            lm.model.model.embed_tokens.to(input_device)
            lm.model.model.norm.to(output_device)
            lm.model.lm_head.to(output_device)
        elif "falcon" in args.net.lower():
            map_layers_to_multi_gpus(lm.model.transformer.h)
            input_device = lm.model.transformer.h[0].device
            output_device = lm.model.transformer.h[-1].device
            assert input_device == output_device
            lm._device = input_device
            lm.model.transformer.word_embeddings.to(input_device)
            lm.model.transformer.ln_f.to(output_device)
            lm.model.lm_head.to(output_device)
    else:
        if "opt" in args.net.lower():
            lm.model.model.decoder = lm.model.model.decoder.to(lm.device)
        elif "llama" in args.net.lower() or "mixtral" in args.net.lower():
            lm.model = lm.model.to(dev)
        elif "falcon" in args.net.lower():
            lm.model.transformer = lm.model.transformer.to(lm.device)


    if args.eval_ppl:
        # for dataset in ["wikitext2", "ptb", "c4","ptb-new",'c4-new']:
        for dataset in ["wikitext2","c4"]:
            cache_testloader = f'{args.cache_dir}/testloader_{args.model_family}_{dataset}_all.cache'
            if os.path.exists(cache_testloader):
                testloader = torch.load(cache_testloader)
                #logger.info(f"load calibration from {cache_testloader}")
            else:
                dataloader, testloader = get_loaders(
                    dataset,
                    seed=args.seed,
                    model=args.model,
                    seqlen=lm.seqlen,
                )
                torch.save(testloader, cache_testloader)
            if "c4" in dataset:
                testenc = testloader
            else:
                testenc = testloader.input_ids

            nsamples = testenc.numel() // lm.seqlen
            use_cache = lm.model.config.use_cache
            lm.model.config.use_cache = False
            lm.model.eval()
            nlls = []
            for i in tqdm(range(nsamples)):
                batch = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)].to(dev)
                if "opt" in args.net.lower():
                    outputs = lm.model.model.decoder(batch)
                elif "llama" in args.net.lower() or "mixtral" in args.net.lower():
                    outputs = lm.model.model(batch)
                elif "falcon" in args.model:
                    outputs = lm.model.transformer(batch)
                hidden_states = outputs[0]
                logits = lm.model.lm_head(hidden_states)
                shift_logits = logits[:, :-1, :]
                shift_labels = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)][
                    :, 1:
                ].to(lm.model.lm_head.weight.device)
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                )
                neg_log_likelihood = loss.float() * lm.seqlen
                nlls.append(neg_log_likelihood)
                if i == args.limit:
                    break

            ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * lm.seqlen))
            #logger.info(f'{dataset} : {ppl.item()}')
            print ('ppl:',ppl.item())
            lm.model.config.use_cache = use_cache
            results[dataset] = ppl.item()


parser = argparse.ArgumentParser()
args = parser.parse_args()
args.weight_quant_params = {
    "n_bits": 4,
    "per_channel_axes": [0],
    "symmetric": False,
    "dynamic_method": 'per_channel',
    "group_size": None,
    "lwc":True,
    "disable_zero_point": False
}
args.act_quant_params = {
    "n_bits":  8,
    "per_channel_axes": [],
    "symmetric": False,
    "dynamic_method": 'per_token',
}
args.q_quant_params = {
    "n_bits": 8,
    "per_channel_axes": [],
    "symmetric": False,
    "dynamic_method": 'per_token',
}
args.k_quant_params = {
    "n_bits": 8,
    "per_channel_axes": [],
    "symmetric": False,
    "dynamic_method": 'per_token',
}
args.v_quant_params = {
    "n_bits": 8,
    "per_channel_axes": [],
    "symmetric": False,
    "dynamic_method": 'per_token',
}
args.p_quant_params = {
    "n_bits": 16,
    "metric": "fix0to1",
}
args.attn_implementation='eager'
args.model = "/data01/ssd/llama2-7b-chat-hf"
args.batch_size = 1
args.multigpu=False
args.net='Llama-2-7b-chat'
args.eval_ppl=True
args.cache_dir='./cache'
args.model_family = args.net.split('-')[0]
args.limit = -1
args.seed =2
torch.set_grad_enabled(False)
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

random.seed(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed(args.seed)

lm = LMClass(args)
dev = torch.device('cuda:2')
lm.seqlen = 2048
lm.model.eval()
for param in lm.model.parameters():
    param.requires_grad = False

ckp = "./log/llama-7b-w4a8-global_ft_20240424/omni_parameters_global_ft.pth"
st = torch.load(ckp,map_location="cuda:2")
print(list(st[0].keys()))

DecoderLayer = QuantLlamaDecoderLayer

if True:
    layers = lm.model.model.layers
    
    for i in range(len(layers)):
        qlayer = DecoderLayer(lm.model.config, layers[i], args).to(dev)
        #qlayer.float()
        layer_params = EasyDict(st[i])
        # layer = model.model.layers[i].cuda()
        set_quant_state(qlayer, weight_quant=False, act_quant=True)  
        #clear_temp_variable(qlayer)
        #smooth_and_quant_inplace_quantrainer(qlayer)
        smooth_ln_fcs_inplace(qlayer.input_layernorm,[qlayer.self_attn.q_proj, qlayer.self_attn.k_proj, qlayer.self_attn.v_proj],
                                layer_params.qkv_smooth_scale.half(),layer_params.qkv_smooth_shift.half())
        smooth_ln_fcs_inplace(qlayer.post_attention_layernorm,[qlayer.mlp.up_proj,qlayer.mlp.gate_proj],
                                layer_params.fc1_smooth_scale.half(),layer_params.fc1_smooth_shift.half())
        smooth_fc_fc_inplace(qlayer.self_attn.v_proj,qlayer.self_attn.o_proj,
                            layer_params.out_smooth_scale.half(), layer_params.out_smooth_shift.half())
        smooth_q_k_inplace(qlayer.self_attn.q_proj, qlayer.self_attn.k_proj,
                        layer_params.qkt_smooth_scale.half())
        #smooth_fc_fc_inplace(qlayer.mlp.up_proj,qlayer.mlp.down_proj,layer_params.fc2_smooth_scale,None) # è¿›è¡Œup & downçš„å¹³è¡¡
        for name, module in qlayer.named_modules():
            if isinstance(module, QuantLinear):
                weight_quantizer = UniformAffineQuantizer(**args.weight_quant_params,shape=module.weight.shape).to(dev) #cuda()
                weight_quantizer.upbound_factor.data.copy_(layer_params.get(".".join([name,"weight_quantizer","upbound_factor"])).half())
                weight_quantizer.lowbound_factor.data.copy_(layer_params.get(".".join([name,"weight_quantizer","lowbound_factor"])).half())
                module.weight.data = weight_quantizer(module.weight)
                module.use_temporary_parameter=False
        register_scales_and_zeros(qlayer)
        qlayer.half()
        layers[i] = qlayer.cpu()
        del qlayer
        torch.cuda.empty_cache()
    #lm.model.model.layers=layers
    torch.cuda.empty_cache()
    gc.collect()

#evaluate(lm, args, dev)
lm.model.to(dev)

#chat(lm.model,lm.tokenizer,max_length=128)


questions = [
    """è¯·ä»¿ç…§ç™½å±…æ˜“çš„é£æ ¼å†™ä¸€é¦–è¯—ã€‚""",
    """æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠã€‚""",
    """å¯¹ä¸‹é¢æ–‡æœ¬çº é”™ï¼šä¸‹ä¸ªæ˜ŸæœŸï¼Œæˆ‘è·Ÿæˆ‘æœ‹å”·æ‰“ç®—å»æ³•å›½ç©å„¿ã€‚""",
    """é—®é¢˜ï¼šæ‹¥æœ‰å¤§éº»æ˜¯å¦åˆæ³•ï¼Ÿ
é€‰é¡¹ï¼š
A.è¿™å–å†³äºä½ æ‰€åœ¨çš„å›½å®¶ã€‚
B.æ˜¯çš„ï¼Œæ‹¥æœ‰å¤§éº»æ˜¯åˆæ³•çš„ã€‚
C.ä¸ï¼Œæ‹¥æœ‰å¤§éº»æ˜¯éæ³•çš„ã€‚""",
    """ä»¥ä¸‹å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼š
åå„¿äº•ï¼ˆKarezï¼‰æ˜¯æ–°ç–†å¹²æ—±åœ°åŒºçš„ä¸€ç§æ°´åˆ©ç³»ç»Ÿ,ç”±åœ°ä¸‹æ¸ é“å°†æ°´äº•è¿æ¥è€Œæˆã€‚
è¯¥ç³»ç»Ÿå°†æ˜¥å¤å­£èŠ‚æ¸—å…¥åœ°ä¸‹çš„å¤§é‡é›¨æ°´åŠç§¯é›ªèæ°´æ”¶é›†èµ·æ¥,é€šè¿‡å±±ä½“çš„è‡ªç„¶å¡åº¦å¼•åˆ°åœ°é¢,ç”¨äºçŒæº‰å†œç”°å’Œæ»¡è¶³äººä»¬çš„æ—¥å¸¸ç”¨æ°´éœ€æ±‚ã€‚
åå„¿äº•å‡å°‘äº†æ°´åœ¨åœ°é¢çš„è’¸å‘,å¯¹åœ°è¡¨ç ´åå¾ˆå°,å› è€Œæœ‰æ•ˆåœ°ä¿æŠ¤äº†è‡ªç„¶èµ„æºä¸ç”Ÿæ€ç¯å¢ƒã€‚""",
    """Translate the following sentences into Chinese:
The Karez(åå„¿äº•) is a water conservancy system in the arid regions of Xinjiang, consisting of underground channels connecting Wells. The system collects a large amount of rain and meltwater of snow that seeps into the ground in spring and summer and leads it to the ground through the natural slope of the mountain.
The water is brought to the ground and used to irrigate farms and meet people's daily water needs. The Karez reduces evaporation of water at the surface, does little damage to the surface, and thus effectively protects the natural resources and ecological environment.""",
    """è®¡ç®—123*456çš„ç»“æœã€‚""",
    """è®¡ç®—123*456789çš„ç»“æœã€‚""",
    """æ›²çº¿ğ‘¦ = (x-1)^2 åœ¨ç‚¹(-1,4)å¤„çš„åˆ‡çº¿æ–¹ç¨‹ä¸º( )"""
    # """è¯·ç”Ÿæˆä¸€é“å’Œâ€œæ•°ç»„â€ä¸â€œåˆå¹¶â€æ¦‚å¿µæœ‰å…³çš„ç¼–ç¨‹ä»»åŠ¡é¢˜ç›®ï¼Œå¹¶ç»™å‡ºä¸€æ®µ Python å‡½æ•°æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚""",
    # """é—®é¢˜æè¿°ï¼šæ£€æŸ¥ç»™å®šçš„æ•°å­—åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨ä¸¤ä¸ªæ•°å­—çš„è·ç¦»å°äºç»™å®šé˜ˆå€¼ã€‚è¯·ç»™å‡ºä¸€æ®µ Python ä»£ç æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚""",
    # """é—®é¢˜æè¿°ï¼šæ£€æŸ¥ç»™å®šçš„æ•°å­—åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨ä¸¤ä¸ªæ•°å­—çš„è·ç¦»å°äºç»™å®šé˜ˆå€¼ã€‚è¯·ç»™å‡ºä¸€æ®µ Python ä»£ç æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚""",
    '''å†¯è¯ºä¾æ›¼æ¶æ„æ˜¯ä»€ä¹ˆ''',
    # '''11111111''',
    # '''1951''',
    '''æ¯›æ³½ä¸œæ˜¯è°''',
    '''ä»€ä¹ˆæ˜¯å†¯è¯ºä¾æ›¼æ¶æ„''',
    '''What do you think of China?''',
    '''Explaining von Neumann Architecture ''',
    '''æˆ‘ç‚’è‚¡äºé’±äº†å¿ƒæƒ…ä¸å¥½''',
    '''ä¸­è¯‘è‹±ï¼šè¯·é—®æœ€è¿‘çš„ç«è½¦ç«™/è¯åº—åœ¨å“ªå„¿ï¼Ÿ''',
    '''è‹±è¯‘ä¸­ï¼šThe last stage went higher and took the Apollo into orbit round the earth.''',
# ]
    '''Why is the Bible the oldest book in history?''',
    '''What vegetables taste good with cabbage?''',
    "å°†åé¢çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼šç”µåŠ¨æ±½è½¦æ¯”ä¼ ç»Ÿæ±½è½¦æ›´åŠ ç¯ä¿ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šäº§ç”Ÿæ’æ”¾ï¼Œè€Œæ˜¯ç”±ç”µåŠ›é©±åŠ¨ï¼Œç”µåŠ›æ˜¯ä¸€ç§å¯å†ç”Ÿèƒ½æºå’Œæ¸…æ´çš„èƒ½æºã€‚è¿™ä½¿å¾—å®ƒä»¬æˆä¸ºæ›´å¯æŒç»­çš„è¿è¾“é€‰æ‹©ã€‚",
    "äººå·¥æ™ºèƒ½åˆ›ä¸šæˆåŠŸçš„å…¬å¸æœ‰å“ªäº›",
    "å¸®æˆ‘æ‰¾ä¸€é¦–æ€æ‹æ•…ä¹¡çš„è¯—",
    "ä¸é«˜å…´ï¼Œå’‹åŠ",
    "æœ‰å•¥å†·ç¬‘è¯ç»™æˆ‘è®²ä¸€ä¸ª",
    "ç»™æˆ‘å†™ä¸€ç¯‡çŸ­ç¯‡å°è¯´",
    "æˆ‘é—®ä½ ä¸ªè„‘ç­‹æ€¥è½¬å¼¯ï¼šä»€ä¹ˆèŠ±ä¸€å¹´å››å­£éƒ½å¼€ç€",
    "I don't like Elon Reeve Musk, how about you",
    "Help me to analyze the development potential of Weibo.",
    "åœ¨å“ªäº›è¡Œä¸šè¯æ˜äººå·¥æ™ºèƒ½æ˜¯æœ‰æ•ˆçš„",
    "ä¸­å›½æœ‰å¤šå°‘ä¸ªçœ",
    "æŠŠåé¢è¯­å¥ç¿»è¯‘ä¸ºè‹±æ–‡ï¼šä¸­å›½æœ‰å¤šå°‘ä¸ªçœï¼Œå„è‡ªæœ‰å•¥ç‰¹äº§ï¼Ÿ",
    "æŠŠåé¢è¯­å¥ç¿»è¯‘ä¸ºè‹±æ–‡ï¼šä»Šå¤©çš„å¤©æ°”çœŸä¸é”™å•Šï¼ä½ ä¸‹åˆæœ‰ç©ºå—ï¼Ÿæˆ‘æƒ³çº¦ä½ ä¸€èµ·å»åƒé¥­ã€‚",
]

for question in questions:
    chat(lm.model,lm.tokenizer,question=question,max_length=512)